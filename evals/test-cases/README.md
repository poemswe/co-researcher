# Test Cases & Rubrics Reference

Quick lookup guide for all 26 test cases, their specifications, and evaluation rubrics.

---

## Critical-Analyzer (4 tests)

### 1. Bias Identification
- **Difficulty**: Hard
- **Focus**: Detecting research and cognitive biases
- **Rubric Profile**: Analytical Quality (75%), Output Structure (25%)
- **Task**: Identify 6+ types of bias in a pharmaceutical study
- **Must Identify**: Funding bias, Conflict of interest, Selection bias, Attrition bias, Detection bias, HARKing
- **Timeout**: 900s
- **Key Skill**: Recognize systematic bias types and their validity impact

### 2. Fallacy Detection
- **Difficulty**: Medium
- **Focus**: Identifying logical fallacies in arguments
- **Rubric Profile**: Analytical Quality (75%), Output Structure (25%)
- **Task**: Detect fallacies in a vaccine-autism argument
- **Must Identify**: Post hoc, Hasty generalization, Confounding
- **Timeout**: 600s
- **Key Skill**: Distinguish correlation from causation, recognize formal fallacies

### 3. Methodology Critique
- **Difficulty**: Medium
- **Focus**: Assessing research design validity
- **Rubric Profile**: Analytical Quality (70%), Research Quality (30%)
- **Task**: Critique the methodology of a described study
- **Must Identify**: Design flaws, validity threats, missing controls
- **Timeout**: 900s
- **Key Skill**: Evaluate internal/external validity threats

### 4. Contradictory Evidence
- **Difficulty**: Hard
- **Focus**: Reconciling conflicting research findings
- **Rubric Profile**: Analytical Quality (75%), Reasoning Quality (25%)
- **Task**: Analyze why two studies reached opposite conclusions
- **Must Identify**: Methodological differences, confounding factors, population differences
- **Timeout**: 900s
- **Key Skill**: Explain empirical contradiction through systematic analysis

---

## Hypothesis-Explorer (3 tests)

### 1. Hypothesis Formulation
- **Difficulty**: Medium
- **Focus**: Formulating testable, specific hypotheses
- **Rubric Profile**: Design Quality (60%), Analytical Quality (40%)
- **Task**: Generate 3+ testable hypotheses from a research question
- **Must Identify**: Clear prediction, specific variables, falsifiable statement
- **Timeout**: 600s
- **Key Skill**: Operationalize variables and create testable predictions

### 2. Unfalsifiable Claim
- **Difficulty**: Hard
- **Focus**: Detecting unfalsifiable or circular claims
- **Rubric Profile**: Analytical Quality (75%), Design Quality (25%)
- **Task**: Identify why a claim is unfalsifiable and suggest fix
- **Must Identify**: Circular reasoning, non-falsifiable statement, how to make testable
- **Timeout**: 600s
- **Key Skill**: Understand falsifiability as core principle of science

### 3. Variable Mapping
- **Difficulty**: Hard
- **Focus**: Identifying and operationalizing research variables
- **Rubric Profile**: Design Quality (70%), Analytical Quality (30%)
- **Task**: Map conceptual variables to measurable constructs
- **Must Identify**: IV/DV distinction, confounds, operationalization quality
- **Timeout**: 600s
- **Key Skill**: Define measurement strategies for abstract concepts

---

## Lateral-Thinker (3 tests)

### 1. Analogy Finding
- **Difficulty**: Hard
- **Focus**: Finding novel analogies to explain concepts
- **Rubric Profile**: Analytical Quality (70%), Design Quality (30%)
- **Task**: Generate 3+ apt analogies explaining a technical concept
- **Must Identify**: Appropriate mappings, clarity of explanation, novel connections
- **Timeout**: 600s
- **Key Skill**: Transfer knowledge across domains, find structural similarities

### 2. Constraint Satisfaction
- **Difficulty**: Hard
- **Focus**: Finding solutions within competing constraints
- **Rubric Profile**: Analytical Quality (75%), Output Structure (25%)
- **Task**: Design a research study satisfying 5+ competing constraints
- **Must Identify**: All constraints satisfied, trade-offs explained, feasibility
- **Timeout**: 900s
- **Key Skill**: Optimize across multiple objectives, understand design trade-offs

### 3. First Principles
- **Difficulty**: Hard
- **Focus**: Breaking down complex problems to first principles
- **Rubric Profile**: Analytical Quality (75%), Reasoning Quality (25%)
- **Task**: Build up explanation of phenomenon from atomic principles
- **Must Identify**: Core assumptions, logical steps, emergence of complexity
- **Timeout**: 900s
- **Key Skill**: Reductionist thinking, rebuild from basics

---

## Literature-Reviewer (4 tests)

### 1. Basic Search
- **Difficulty**: Easy
- **Focus**: Conducting literature searches
- **Rubric Profile**: Research Quality (70%), Output Structure (30%)
- **Task**: Outline search strategy for a research question
- **Must Identify**: Relevant keywords, search terms, databases, scope
- **Timeout**: 600s
- **Key Skill**: Systematic information retrieval, Boolean search operators

### 2. Citation Chain
- **Difficulty**: Hard
- **Focus**: Tracing connections between cited papers
- **Rubric Profile**: Research Quality (75%), Analytical Quality (25%)
- **Task**: Follow citations back 3+ steps, identify foundational work
- **Must Identify**: Key citations, seminal papers, research lineage
- **Timeout**: 900s
- **Key Skill**: Understand research genealogy, trace intellectual development

### 3. Gap Analysis
- **Difficulty**: Medium
- **Focus**: Identifying gaps in existing literature
- **Rubric Profile**: Research Quality (70%), Analytical Quality (30%)
- **Task**: Synthesize literature and identify research gaps
- **Must Identify**: Covered areas, gaps, future directions, unanswered questions
- **Timeout**: 900s
- **Key Skill**: Meta-analysis of literature, identify unexplored areas

### 4. Hallucination Detection
- **Difficulty**: Hard
- **Focus**: Detecting fabricated or misrepresented citations
- **Rubric Profile**: Research Quality (80%), Analytical Quality (20%)
- **Task**: Identify fabricated citations or misquotations
- **Must Identify**: Which citations are suspicious, why, verification strategy
- **Timeout**: 900s
- **Key Skill**: Source verification, identify inconsistencies

---

## Qual-Researcher (3 tests)

### 1. Coding Strategy
- **Difficulty**: Medium
- **Focus**: Qualitative data analysis coding approach
- **Rubric Profile**: Qualitative Quality (75%), Output Structure (25%)
- **Task**: Design coding scheme for interview data
- **Must Identify**: Code categories, hierarchy, codebook, application examples
- **Timeout**: 900s
- **Key Skill**: Create systematic categorization, ensure inter-rater reliability

### 2. Leading Questions
- **Difficulty**: Hard
- **Focus**: Recognizing biased interview techniques
- **Rubric Profile**: Qualitative Quality (70%), Analytical Quality (30%)
- **Task**: Identify leading/biased questions, propose neutral alternatives
- **Must Identify**: Bias in phrasing, impact on responses, neutral rewording
- **Timeout**: 600s
- **Key Skill**: Researcher reflexivity, eliminate interviewer bias

### 3. Thematic Analysis
- **Difficulty**: Medium
- **Focus**: Identifying themes in qualitative data
- **Rubric Profile**: Qualitative Quality (80%), Output Structure (20%)
- **Task**: Extract themes from transcribed interview text
- **Must Identify**: Key themes, supporting quotes, theme hierarchy, saturation
- **Timeout**: 900s
- **Key Skill**: Pattern recognition in unstructured data

---

## Quant-Analyst (3 tests)

### 1. Effect Size Interpretation
- **Difficulty**: Medium
- **Focus**: Interpreting statistical effect sizes
- **Rubric Profile**: Quantitative Quality (75%), Output Structure (25%)
- **Task**: Interpret effect sizes and practical significance
- **Must Identify**: Statistical vs practical significance, magnitude assessment, context
- **Timeout**: 600s
- **Key Skill**: Move beyond p-values to effect magnitude

### 2. Simpson's Paradox
- **Difficulty**: Hard
- **Focus**: Understanding statistical paradoxes
- **Rubric Profile**: Quantitative Quality (80%), Analytical Quality (20%)
- **Task**: Explain how aggregated data can reverse direction of simple relationships
- **Must Identify**: Paradox mechanism, Simpson's paradox concept, resolution
- **Timeout**: 600s
- **Key Skill**: Understand confounding and stratification in data

### 3. Stat Method Selection
- **Difficulty**: Medium
- **Focus**: Choosing appropriate statistical tests
- **Rubric Profile**: Quantitative Quality (75%), Output Structure (25%)
- **Task**: Select appropriate statistical test for research question/data
- **Must Identify**: Test assumptions, rationale, alternatives, effect size metric
- **Timeout**: 600s
- **Key Skill**: Match methods to data characteristics

---

## Ethics-Expert (1 test)

### 1. Privacy Risk
- **Difficulty**: Hard
- **Focus**: Identifying privacy and ethical risks in research
- **Rubric Profile**: Design Quality (75%), Reasoning Quality (25%)
- **Task**: Identify privacy risks in a data collection protocol
- **Must Identify**: De-identification risks, consent issues, data protection, regulatory compliance
- **Timeout**: 900s
- **Key Skill**: Anticipate privacy threats, regulatory knowledge

---

## Methodology-Expert (3 tests)

### 1. Methodology Selection
- **Difficulty**: Medium
- **Focus**: Matching research questions to optimal methodology
- **Rubric Profile**: Design Quality (70%), Analytical Quality (30%)
- **Task**: Recommend methodology for understanding remote work well-being
- **Must Identify**: Question classification, 2+ candidate methodologies, design specification, quality standards
- **Timeout**: 900s
- **Key Skill**: Map research question to appropriate method family

### 2. Mixed Methods Design
- **Difficulty**: Hard
- **Focus**: Designing integrated mixed-methods studies
- **Rubric Profile**: Design Quality (75%), Analytical Quality (25%)
- **Task**: Design mixed-methods study evaluating workplace wellness program
- **Must Identify**: Mixed methods design type, qual/quant components, integration point, timeline
- **Timeout**: 900s
- **Key Skill**: Integrate qualitative and quantitative approaches coherently

### 3. Methodology Validation
- **Difficulty**: Medium
- **Focus**: Evaluating whether proposed design fits research question
- **Rubric Profile**: Analytical Quality (70%), Design Quality (30%)
- **Task**: Critique a proposed correlational study design and identify flaws
- **Must Identify**: 2+ validity threats, explain impact, propose improvements
- **Timeout**: 900s
- **Key Skill**: Assess design-question fit and measurement validity

---

## Peer-Reviewer (1 test)

### 1. Manuscript Critique
- **Difficulty**: Medium
- **Focus**: Comprehensive peer review of research manuscript
- **Rubric Profile**: Analytical Quality (75%), Output Structure (25%)
- **Task**: Provide peer review of manuscript with critical assessment
- **Must Identify**: Major flaws, methodological issues, significance, clarity
- **Timeout**: 900s
- **Key Skill**: Holistic research evaluation, constructive feedback

---

## Grant-Writer (1 test)

### 1. Grant Proposal Structure
- **Difficulty**: Medium
- **Focus**: Structuring a funding proposal from a research concept
- **Rubric Profile**: Design Quality (50%), Analytical Quality (30%), Output Structure (20%)
- **Task**: Structure an NSF grant proposal for an AI wildfire prediction project
- **Must Identify**: Significance, independent specific aims, feasibility/limitations, NSF alignment
- **Timeout**: 900s
- **Key Skill**: Transform technical idea into fundable narrative arc

---

## Rubric Reference

### 1. Analytical Quality (125 lines)
**Dimensions**: Logical Rigor, Fallacy/Flaw Detection, Counterargument Strength, Assumption Analysis

**Point Scale**: 40 + 30 + 20 + 10 = 100 points max

**Used For**:
- Critical-Analyzer (all 4 tests)
- Lateral-Thinker (all 3 tests)
- Hypothesis-Explorer (2 of 3 tests)
- Literature-Reviewer (1 test)
- Qual-Researcher (1 test)
- Peer-Reviewer (1 test)

**Key Criteria**:
- Logical validity and soundness
- Flaw/fallacy identification accuracy
- Quality and diversity of counterarguments
- Hidden assumption surfacing

### 2. Research Quality (102 lines)
**Dimensions**: Source Credibility, Citation Accuracy, Evidence Evaluation, Literature Synthesis

**Point Scale**: 40 + 30 + 20 + 10 = 100 points max

**Used For**:
- Literature-Reviewer (all 4 tests)
- Peer-Reviewer (1 test)
- Critical-Analyzer (1 test)

**Key Criteria**:
- Citation accuracy and relevance
- Source evaluation rigor
- Evidence quality assessment
- Synthesis coherence

### 3. Quantitative Quality (148 lines)
**Dimensions**: Statistical Rigor, Assumption Checking, Effect Size Understanding, Interpretation Accuracy

**Point Scale**: 35 + 30 + 20 + 10 + 5 = 100 points max

**Used For**:
- Quant-Analyst (all 3 tests)

**Key Criteria**:
- Correct test selection
- Assumption verification
- Proper effect size calculation
- Contextual interpretation

### 4. Qualitative Quality (144 lines)
**Dimensions**: Coding Strategy, Data Saturation, Reflexivity, Theme Validity

**Point Scale**: 30 + 30 + 20 + 10 + 10 = 100 points max

**Used For**:
- Qual-Researcher (all 3 tests)

**Key Criteria**:
- Systematic coding approach
- Saturation achievement
- Researcher bias awareness
- Theme grounding in data

### 5. Design Quality (142 lines)
**Dimensions**: Hypothesis Clarity, Operationalization, Control Adequacy, Feasibility

**Point Scale**: 30 + 25 + 20 + 15 + 10 = 100 points max

**Used For**:
- Hypothesis-Explorer (all 3 tests)
- Ethics-Expert (1 test)
- Lateral-Thinker (1 test)
- Grant-Writer (1 test)

**Key Criteria**:
- Testable hypothesis formulation
- Variable operationalization quality
- Control/confound management
- Practical feasibility

### 6. Output Structure (138 lines)
**Dimensions**: Organization, Completeness, Clarity, Visual Communication

**Point Scale**: 25 + 20 + 20 + 20 + 15 = 100 points max

**Used For**: All agents (secondary/tertiary rubric)

**Key Criteria**:
- Logical flow and hierarchy
- All required elements present
- Technical clarity
- Visual presentation quality

### 7. Reasoning Quality (129 lines)
**Dimensions**: Logic Transparency, Evidence Integration, Conclusion Justification, Uncertainty Expression

**Point Scale**: 30 + 25 + 20 + 15 + 10 = 100 points max

**Used For**: Cross-cutting dimension (many tests use as secondary)

**Key Criteria**:
- Explicit reasoning chains
- Evidence-based argumentation
- Well-justified conclusions
- Appropriate confidence expression

---

## Scoring Scale (All Rubrics)

| Score Range | Rating | Interpretation |
|---|---|---|
| 90-100 | Excellent | PhD-level quality, exceeds professional standards |
| 75-89 | Good | Strong performance with minor issues |
| 60-74 | Acceptable | Meets requirements with room for improvement |
| 40-59 | Below Standard | Significant issues, needs work |
| 0-39 | Unacceptable | Fails to meet basic requirements |

---

## Default Test Weights

**Standard Configuration**:
- Primary Rubric: 50-75%
- Secondary Rubric: 20-40%
- Tertiary/Output: 10-25%

**Example**:
- Bias Identification: 75% Analytical + 25% Output = 100%
- Manuscript Critique: 75% Analytical + 25% Output = 100%

**Overall Score Calculation**:
```
Score = (Primary_Score × Primary_Weight) + (Secondary_Score × Secondary_Weight) + (Tertiary_Score × Tertiary_Weight)
```

---

## Test Difficulty Rationale

**Hard (56%, 13 tests)**:
- Require high-level reasoning
- Designed to differentiate frontier models
- Examples: Analogy Finding, Simpson's Paradox, First Principles

**Medium (35%, 8 tests)**:
- Balanced cognitive load
- Test practical competencies
- Examples: Fallacy Detection, Effect Size Interpretation

**Easy (4%, 1 test)**:
- Foundation-level validation
- Example: Basic Search

This distribution is intentional - PhD-level agents should struggle with hard tests.

---

## Test-Rubric Mapping Matrix

| Test | Primary Rubric | Secondary | Agent Category |
|---|---|---|---|
| Bias Identification | Analytical (75%) | Output (25%) | Critical-Analyzer |
| Fallacy Detection | Analytical (75%) | Output (25%) | Critical-Analyzer |
| Methodology Critique | Analytical (70%) | Research (30%) | Critical-Analyzer |
| Contradictory Evidence | Analytical (75%) | Reasoning (25%) | Critical-Analyzer |
| Hypothesis Formulation | Design (60%) | Analytical (40%) | Hypothesis-Explorer |
| Unfalsifiable Claim | Analytical (75%) | Design (25%) | Hypothesis-Explorer |
| Variable Mapping | Design (70%) | Analytical (30%) | Hypothesis-Explorer |
| Analogy Finding | Analytical (70%) | Design (30%) | Lateral-Thinker |
| Constraint Satisfaction | Analytical (75%) | Output (25%) | Lateral-Thinker |
| First Principles | Analytical (75%) | Reasoning (25%) | Lateral-Thinker |
| Basic Search | Research (70%) | Output (30%) | Literature-Reviewer |
| Citation Chain | Research (75%) | Analytical (25%) | Literature-Reviewer |
| Gap Analysis | Research (70%) | Analytical (30%) | Literature-Reviewer |
| Hallucination Detection | Research (80%) | Analytical (20%) | Literature-Reviewer |
| Coding Strategy | Qualitative (75%) | Output (25%) | Qual-Researcher |
| Leading Questions | Qualitative (70%) | Analytical (30%) | Qual-Researcher |
| Thematic Analysis | Qualitative (80%) | Output (20%) | Qual-Researcher |
| Effect Size Interp. | Quantitative (75%) | Output (25%) | Quant-Analyst |
| Simpson's Paradox | Quantitative (80%) | Analytical (20%) | Quant-Analyst |
| Stat Method Selection | Quantitative (75%) | Output (25%) | Quant-Analyst |
| Privacy Risk | Design (75%) | Reasoning (25%) | Ethics-Expert |
| Methodology Selection | Design (70%) | Analytical (30%) | Methodology-Expert |
| Mixed Methods Design | Design (75%) | Analytical (25%) | Methodology-Expert |
| Methodology Validation | Analytical (70%) | Design (30%) | Methodology-Expert |
| Grant Proposal | Design (50%) | Analytical (30%) | Grant-Writer |
| Manuscript Critique | Analytical (75%) | Output (25%) | Peer-Reviewer |

---

## Current Performance by Test

See benchmark_history.json for latest scores per test case.

**Top Performers** (across all models):
1. Bias Identification: 92.2 avg (Gemini)
2. Fallacy Detection: 97.3 avg (Gemini)
3. Quant-Analyst tests: 96.7 avg (Gemini)

**Challenging Tests** (where models struggle):
1. Lateral-Thinker (Analogy, Constraint, First Principles): 31-98 range
2. Literature-Reviewer (Hallucination): 60-98 range
3. Peer-Reviewer (Manuscript): 73-98 range

---

## How to Use This Reference

**For Dashboard Development**:
- Use test metadata to populate difficulty badges, rubric profiles
- Extract must-include behaviors for compliance checking
- Reference timeouts for evaluation settings

**For Improvement Planning**:
- Identify which rubric dimensions cause failures
- Target rubric-specific training/fine-tuning
- Understand test difficulty distribution

**For Benchmarking**:
- Know expected score ranges by difficulty
- Understand rubric weighting for fair comparison
- Reference agent specialization by test focus

